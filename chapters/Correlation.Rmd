---
title: "Corrleation"
output: md_document
---

<script type="text/javascript" async
    src="https://polyfill.io/v3/polyfill.min.js?features=es6">
</script>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
</script>

# Correlation Analysis

## Introduction
A *correlation analysis* is a statistical method used to examine the strength and direction of the relationship between two continuous variables. 

### Pearson Correlation Calculation
The Pearson correlation coefficient is calculated using:

$$
\large r = \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
$$

where:

* $x_{i}$ and $y_{i}$ are the individual sample points.
* $\bar{x}$ and $\bar{y}$ are means of $x$ and $y$.
* $\sum (x_i - \bar{x})$ $\sum (y_i - \bar{y})$ is the cross-product of deviations.
* $\sum (x_i - \bar{x})^2$ and \sum (y_i - \bar{y})^2 are the sum of of squared deviations.

The most commonly used correlation coefficient is **Pearson's correlation coefficient**, which measures the linear relationship between two variables. Pearson's correlation coefficient, denoted by "r," takes on values between -1 and 1, where a value of -1 indicates a perfect negative correlation, a value of 0 indicates no correlation, and a value of 1 indicates a perfect positive correlation.

$$
\large -1 \leq r \leq 1
$$

where:

- \( r = -1 \) indicates a perfect negative correlation,
- \( r = 0 \) indicates no correlation,
- \( r = 1 \) indicates a perfect positive correlation.

To perform a correlation analysis, we first must collect data on the two variables of interest. Once we have the data, we can calculate the correlation coefficient using R.

For example, suppose we want to examine the relationship between a person's age and their cholesterol level. We collect data on 100 individuals and obtained the following data:

## Example Dataset
We will examine the relationship between a person's age and their cholesterol level using sample data:

```{r}
# Load necessary libraries
library(ggplot2)

# Create age and cholesterol vectors
age <- c(45, 38, 52, 60, 35, 42, 48, 55, 50, 47)
cholesterol <- c(210, 185, 240, 250, 175, 200, 220, 235, 230, 210)

# Combine the vectors into a dataframe
mydata <- data.frame(age = age, cholesterol = cholesterol)
```

### Data Representation
```{r, echo=FALSE}
library(knitr)
kable(mydata)
```

## Scatter Plot
```{r}
ggplot(mydata, aes(x = age, y = cholesterol)) + 
  geom_point(color = "blue", size = 3) + 
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Relationship between Age and Cholesterol", x = "Age", y = "Cholesterol") +
  theme_classic()
```

## Assumptions of Pearson Correlation
### 1. Normality Check

#### Histogram of Age
```{r}
ggplot(mydata, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Age", x = "Age", y = "Frequency") +
  theme_classic()
```

#### Histogram of Cholesterol
```{r}
ggplot(mydata, aes(x = cholesterol)) +
  geom_histogram(binwidth = 20, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Cholesterol", x = "Cholesterol", y = "Frequency") +
  theme_classic()
```

### Shapiro-Wilk Test
The Shapiro-Wilk test is a statistical test used to check the normality assumption of a population or a sample. It was developed by Samuel Shapiro and Martin Wilk in 1965.

The null hypothesis of the Shapiro-Wilk test is that the population is normally distributed. The alternative hypothesis is that the population is not normally distributed.

The test is based on the correlation between the data and the normal probability plot (also known as the normal quantile plot). The normal probability plot is a graphical method to check the normality assumption. It plots the observed data against the expected values from a normal distribution. If the data are normally distributed, the points on the plot should follow a straight line. 

The Shapiro-Wilk test is widely used in many fields, such as psychology, biology, economics, and finance, to check the normality assumption before applying parametric statistical methods that assume normality, such as t-tests and ANOVA.

There are several alternative tests that can be used to assess normality assumptions, depending on the type of data and the research question. Here are a few examples: Anderson-Darling test, Kolmogorov-Smirnov test, Normal probability plots, etc

#### Shapiro-Wilk Test for Normality
```{r}
shapiro.test(mydata$age) 
shapiro.test(mydata$cholesterol)
```

### 2. Linearity Check
```{r}
ggplot(mydata, aes(x = age, y = cholesterol)) + 
  geom_point(color = "blue", size = 3) + 
  geom_smooth(method = "lm", col = "red") +
  labs(title = "Linearity Check", x = "Age", y = "Cholesterol") +
  theme_classic()
```

```{r, echo=FALSE}
library(knitr)
kable(mydata)
```

**Step 1: Compute the Means**

The mean of a dataset is calculated as:

$$
\bar{x} = \frac{\sum x_i}{n}, \quad \bar{y} = \frac{\sum y_i}{n}
$$

For our dataset:

$$
\bar{x} = \frac{45+38+52+60+35+42+48+55+50+47}{10} = \frac{472}{10} = 47.2
$$

$$
\bar{y} = \frac{210+185+240+250+175+200+220+235+230+210}{10} = \frac{2155}{10} = 215.5
$$

---

**Step 2: Compute Deviations and Squared Deviations**

| Age (\(x_i\)) | Cholesterol (\(y_i\)) | \(x_i - \bar{x}\) | \(y_i - \bar{y}\) | \((x_i - \bar{x})^2\) | \((y_i - \bar{y})^2\) | \((x_i - \bar{x})(y_i - \bar{y})\) |
|:-------------|:---------------|:---------------|:---------------|:-----------------|:-----------------|:---------------------|
| 45          | 210            | -2.2           | -5.5           | 4.84             | 30.25            | 12.1                 |
| 38          | 185            | -9.2           | -30.5          | 84.64            | 930.25           | 280.6                |
| 52          | 240            | 4.8            | 24.5           | 23.04            | 600.25           | 117.6                |
| 60          | 250            | 12.8           | 34.5           | 163.84           | 1190.25          | 441.6                |
| 35          | 175            | -12.2          | -40.5          | 148.84           | 1640.25          | 494.1                |
| 42          | 200            | -5.2           | -15.5          | 27.04            | 240.25           | 80.6                 |
| 48          | 220            | 0.8            | 4.5            | 0.64             | 20.25            | 3.6                  |
| 55          | 235            | 7.8            | 19.5           | 60.84            | 380.25           | 152.1                |
| 50          | 230            | 2.8            | 14.5           | 7.84             | 210.25           | 40.6                 |
| 47          | 210            | -0.2           | -5.5           | 0.04             | 30.25            | 1.1                  |


---

**Step 3: Compute Summations**

$$
\sum (x_i - \bar{x}) (y_i - \bar{y} ) = 1624.3
$$

$$
\sum (x_i - \bar{x})^2 = 521.76
$$

$$
\sum (y_i - \bar{y})^2 = 5272.5
$$

---

**Step 4: Compute Pearson Correlation Coefficient**

The Pearson correlation coefficient is calculated as:

$$
r = \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
$$

Substituting values:

$$
r = \frac{1624.3}{\sqrt{521.76 \times 5272.5}}
$$

$$
r = \frac{1624.3}{\sqrt{2751143.6}}
$$

$$
r = \frac{1624.3}{1658.6}
$$

$$
r = 0.98
$$

---

**Step 5: Statistical Significance Test**

We test the null hypothesis:

$$
H_0: \text{There is no correlation} \quad (r = 0)
$$


The test statistic is calculated using:

$$
t = \frac{r \sqrt{n-2}}{\sqrt{1 - r^2}}
$$

Substituting values:

$$
t = \frac{0.98 \times \sqrt{10-2}}{\sqrt{1 - 0.98^2}}
$$

$$
t = \frac{0.98 \times \sqrt{8}}{\sqrt{1 - 0.9604}}
$$

$$
t = \frac{0.98 \times 2.83}{\sqrt{0.0396}}
$$

$$
t = \frac{2.77}{0.199}
$$

$$
t = 13.94
$$

* The degrees of freedom (df) for Pearson correlation significance testing is calculated using the formula:


* df = n - 2


where:
* n is the number of observations in the dataset.

* The Pearson correlation formula includes **two estimated means** ($\bar{x}$ and $\bar{y}$, which reduces the independent data points available for variability estimation.
* The correlation coefficient relies on **bivariate relationships**, and removing **two degrees of freedom** accounts for the constraints imposed by using means in the calculations.

Using a **t-distribution table for 8 degrees of freedom** at \( \alpha = 0.05 \):

$$
t_{cri} = 2.306
$$

* cri: critical

Since \( t = 13.94 > 2.306 \), we **reject** \( H_0 \) and conclude that the correlation is **statistically significant**.


**Performing Pearson Correlation in R**
```{r}
cor(mydata$age, mydata$cholesterol)
```

### Statistical Significance Test
```{r}
cor.test(mydata$age, mydata$cholesterol)
```

## Spearman Rank Correlation
The spearman rank correlation is a non-parametric statistical method used to measure the strength and direction of the association between two variables. It is calculated by ranking the values of each variable and calculating the correlation coefficient between their ranks. It is robust to outliers and can be used with ordinal or non-normally distributed data.

Let's say we want to investigate the relationship between the amount of hours that students spend studying for a test and their corresponding test scores. We collect data from 10 students and get the following results:


$$
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
$$

where:

- \( d_i \) = difference between ranks of corresponding values.
- \( n \) = number of observations.
- \( \sum d_i^2 \) = sum of squared rank differences.

---

**Step 1: Given Dataset**

| Hours Studied (\(x\)) | Test Score (\(y\)) |
|----------------------|------------------|
| 2  | 60  |
| 3  | 70  |
| 5  | 80  |
| 6  | 85  |
| 7  | 90  |
| 8  | 95  |
| 9  | 98  |
| 10 | 100 |
| 12 | 99  |
| 14 | 95  |

---

**Step 2: Assign Ranks**

We rank both variables from lowest to highest.

| Hours Studied (\(x_i\)) | Rank (\(R_x\)) | Test Score (\(y_i\)) | Rank (\(R_y\)) |
|----------------------|----------------|------------------|----------------|
| 2  | 1  | 60  | 1  |
| 3  | 2  | 70  | 2  |
| 5  | 3  | 80  | 3  |
| 6  | 4  | 85  | 4  |
| 7  | 5  | 90  | 5  |
| 8  | 6  | 95  | 7  |
| 9  | 7  | 98  | 8  |
| 10 | 8  | 100 | 9  |
| 12 | 9  | 99  | 10  |
| 14 | 10 | 95  | 6  |

---

**Step 3: Compute Rank Differences and Squared Differences**

The difference in ranks is:

$$
d_i = R_x - R_y
$$

The squared difference is:

$$
d_i^2 = (R_x - R_y)^2
$$

| Hours Studied (\(x_i\)) | Rank (\(R_x\)) | Test Score (\(y_i\)) | Rank (\(R_y\)) | \( d_i \) | \( d_i^2 \) |
|:----------------------|:----------------|:------------------|:----------------|:------|:------|
| 2  | 1  | 60  | 1  | 0  | 0  |
| 3  | 2  | 70  | 2  | 0  | 0  |
| 5  | 3  | 80  | 3  | 0  | 0  |
| 6  | 4  | 85  | 4  | 0  | 0  |
| 7  | 5  | 90  | 5  | 0  | 0  |
| 8  | 6  | 95  | 7  | -1 | 1  |
| 9  | 7  | 98  | 8  | -1 | 1  |
| 10 | 8  | 100 | 9  | -1 | 1  |
| 12 | 9  | 99  | 10 | -1 | 1  |
| 14 | 10 | 95  | 6  | 4  | 16 |

---

## **Step 4: Compute Summation**

$$
\sum d_i^2 = 0 + 0 + 0 + 0 + 0 + 1 + 1 + 1 + 1 + 16 = 20
$$

---

## **Step 5: Compute Spearman Correlation**

Using the formula:

$$
\rho = 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
$$

Substituting values:

$$
\rho = 1 - \frac{6 \times 20}{10(10^2 - 1)}
$$

$$
\rho = 1 - \frac{120}{10(100 - 1)}
$$

$$
\rho = 1 - \frac{120}{10 \times 99}
$$

$$
\rho = 1 - \frac{120}{990}
$$

$$
\rho = 1 - 0.1212
$$

$$
\rho = 0.8788
$$

---

**Step 6: Statistical Significance Test**

We test the null hypothesis:

$$
H_0: \text{There is no correlation} \quad (\rho = 0)
$$

The test statistic is:

$$
t = \frac{\rho \sqrt{n - 2}}{\sqrt{1 - \rho^2}}
$$

Substituting values:

$$
t = \frac{0.8788 \times \sqrt{10-2}}{\sqrt{1 - 0.8788^2}}
$$

$$
t = \frac{0.8788 \times \sqrt{8}}{\sqrt{1 - 0.7722}}
$$

$$
t = \frac{0.8788 \times 2.83}{\sqrt{0.2278}}
$$

$$
t = \frac{2.487}{0.4773}
$$

$$
t = 5.21
$$

Using a **t-distribution table for 8 degrees of freedom** at \( \alpha = 0.05 \):

$$
t_{cri} = 2.306
$$

* cri: critical

Since \( t = 5.21 > 2.306 \), we **reject** \( H_0 \) and conclude that the correlation is **statistically significant**.

---


```{r echo=FALSE}
# Creating two vectors
Hr_St <- c(2, 3, 5, 6, 7, 8, 9, 10, 12, 14)
Test_score <- c(60, 70, 80, 85, 90, 95, 98, 100, 99, 95)

# Combine the vectors into a dataframe
mydata <- data.frame(Hr_St = Hr_St, Test_score = Test_score)
```


### Spearman Correlation Calculation using R
```{r}
cor.test(mydata$Hr_St, mydata$Test_score, method = "spearman")
```

## Conclusion
- Pearson's correlation coefficient is used for linear relationships assuming normality.
- Spearman's correlation is robust to non-normal distributions and outliers.
- The results indicate a **strong positive correlation** in both cases, implying a significant association between the variables.


#### References
1. Pearson, K. (1895). "Notes on Regression and Inheritance in the Case of Two Parents." *Proceedings of the Royal Society of London*, 58(1), 240–242.
2. Spearman, C. (1904). "The Proof and Measurement of Association between Two Things." *The American Journal of Psychology*, 15(1), 72–101.
3. Shapiro, S. S., & Wilk, M. B. (1965). "An Analysis of Variance Test for Normality (Complete Samples)." *Biometrika*, 52(3-4), 591–611.
4. Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences*. Routledge.

[⬅ Back to Home](../index.md)